{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> What's Policy Learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy** is a function which maps current observation to an action. It is more direct way for an Agent to learn *good behaviour* by acting in the environment. The other way, **Q Learning**, learns a quantified value for being in a particular state $s$ and performing a particular action $a$. That helps in exploration, PL doesn't have that much of foresightedness since it doesn't consider long term rewards in choosing policy *(No Bellman equation)*. \n",
    "\n",
    "So, Policy Learning may be used for environments where reward is based on immediate action *(non-hysterical)*. \n",
    "\n",
    "$$\\pi(s) \\rightarrow a$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Toy application - Part I\n",
    "\n",
    "Let's apply Policy learning to a **4-Armed Bandit problem**. 4-Armed Bandit is a slot machine with four levers. \n",
    "On pulling a lever, you might get some reward. These rewards vary for ever lever (fixed probabilities that are preprogrammed). Our goal is to find the best lever to pull in-order to maximize our total reward.\n",
    "\n",
    "**Choosing an action** is like picking a lever to pull. So, we have 4 possible actions. \n",
    "Each action should be associated with a weight. This weight determines how ofter that action will be chosen. Thus, scrutinizing policy function.\n",
    "\n",
    "**Loss function** should take reward & action into account and modify weight of action to re-establish the ground truth in Agent's knowledge base.\n",
    "\n",
    "If $A$ is reward we get (called advantage) for $\\pi$, the action we chose; \n",
    "$$loss = -\\log(\\pi) \\cdot A$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ENDED\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Ground-truth about the 4 levers\n",
    "bandits = [4, 2.5, -4, 6]  # As per our reward fn., Bandit_id 2 yields max reward\n",
    "\n",
    "# Let's define reward function\n",
    "def get_reward(bandit_id):\n",
    "    \"\"\" Takes Bandit index as input and returns 1 or -1 based on pre-established probability \"\"\"\n",
    "    \n",
    "    if bandits[bandit_id] < np.random.randn():\n",
    "        \n",
    "        return 1\n",
    "    else:\n",
    "        \n",
    "        return -1\n",
    "    \n",
    "\n",
    "# Neural Agent to learn that bandits policy function\n",
    "policy = tf.Variable(np.ones([len(bandits)]))\n",
    "get_action = tf.argmax(policy, 0)\n",
    "reward = tf.placeholder(shape=1, dtype=tf.float64)\n",
    "action = tf.placeholder(shape=1, dtype=tf.int32)               \n",
    "policy_function_weight = tf.slice(policy, action, [1])  # Get the weight related to policy[action]\n",
    "loss = -tf.log(policy_function_weight) * reward\n",
    "                     \n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "learn = trainer.minimize(loss)                     \n",
    "\n",
    "\n",
    "# Setting up env and training Agent\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "episodes = 2000\n",
    "all_rewards = []\n",
    "\n",
    "\n",
    "for _ in range(episodes):\n",
    "    accumulated_reward = 0\n",
    "    action_ = sess.run(get_action)\n",
    "    \n",
    "    # Greedily pick a random action now and then <exploration>\n",
    "    if np.random.rand() < 0.2:\n",
    "        action_ = np.random.randint(4)\n",
    "        \n",
    "    reward_ = get_reward(action_)\n",
    "    \n",
    "    sess.run(learn, {action: [action_], reward: [reward_]})\n",
    "    \n",
    "    accumulated_reward += reward_\n",
    "    all_rewards.append(accumulated_reward)\n",
    "    \n",
    "    \n",
    "print \"TRAINING ENDED\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.90559323,  0.89560604,  2.09731758,  0.8809805 ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what the network has learned\n",
    "sess.run(policy)\n",
    "\n",
    "# Notice that 2nd bandit's policy value is the highest; which it should be\n",
    "# Also notice that, since we were exploring a bit, the other policy values aligned to their \n",
    "# respective returns with 3rd index scoring least possible value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff6c53f9890>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff6c7756890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(all_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "1. What happens if we randomly pick actions more often?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Application - Part II\n",
    "\n",
    "Now a simpler expansion of this problem is to include *state dependence*. \n",
    "Part I dealt with simply learning policy assuming always in default state. Part II can be extended to have multiple slot machines, each with four levers to pull and the Agent has to figure out which slot-machine it has to choose and which lever to pull among the chosen - in a way that it maximizes the total reward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Ended\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class BanditEnv(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Ground-truth slot machine probabilites\n",
    "        self.slot_machines = [[+1, +5, +8, -1], \n",
    "                              [+2, -9, +0, -2], \n",
    "                              [-1, -1, -1, +1]]\n",
    "        \n",
    "        \n",
    "    def get_reward(self, state, action):\n",
    "        \n",
    "        if self.slot_machines[state][action] < np.random.randn():\n",
    "            return +1\n",
    "        \n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "        \n",
    "# Neural Agent modelling\n",
    "\n",
    "# Agent's representation of model\n",
    "agent_env = tf.Variable(np.ones((3, 4)))\n",
    "state = tf.placeholder(dtype=tf.int32)\n",
    "action = tf.placeholder(dtype=tf.int32)\n",
    "reward = tf.placeholder(dtype=tf.float64)\n",
    "get_action = tf.argmax(agent_env)\n",
    "agent_env_weight = tf.slice(agent_env, [state, action], [1, 1])\n",
    "loss = -tf.log(agent_env_weight) * reward\n",
    "\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "learn = trainer.minimize(loss)\n",
    "        \n",
    "        \n",
    "# Training the agent\n",
    "env = BanditEnv()\n",
    "sess1 = tf.Session()\n",
    "sess1.run(tf.global_variables_initializer())\n",
    "episodes = 2000\n",
    "\n",
    "for _ in range(episodes):\n",
    "    \n",
    "    # Get started by picking a random state \n",
    "    state_ = np.random.randint(3)\n",
    "    \n",
    "    # Get action for that state and chose greedily now and then\n",
    "    action_ = sess1.run(get_action)[state_]\n",
    "    if np.random.randn() < 0.3:\n",
    "        action_ = np.random.randint(4)\n",
    "    \n",
    "    # Get reward for the duo\n",
    "    reward_ = env.get_reward(state_, action_)\n",
    "    #print sess1.run(loss, {reward: reward_, state: state_, action: action_})\n",
    "    #print sess1.run(agent_env_weight, {reward: reward_, state: state_, action: action_})\n",
    "    \n",
    "    # Digest the reward_ and update agent_env\n",
    "    sess1.run(learn, {reward: reward_, state: state_, action: action_})\n",
    "    \n",
    "print \"Training Ended\"\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.53011634, -0.38827324, -3.76474276,  1.57705525],\n",
       "       [-2.46210834,  2.87238112,  1.01450545,  1.81829078],\n",
       "       [ 1.589473  ,  1.59604155,  2.34793614, -5.75765858]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what the network has learned\n",
    "sess1.run(agent_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO \n",
    "\n",
    "1. See what happens if you don't greedily choose random actions once in a while"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
