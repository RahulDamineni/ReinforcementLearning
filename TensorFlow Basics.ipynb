{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Learning TensorFlow Basics </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts:\n",
    "\n",
    "    1. Computational graph\n",
    "    2. Nodes in graph are madeup of tensors\n",
    "    3. Tensors are multidimensional arrays and basic datatype of TF\n",
    "    4. Graph should be built by declaring and connecting tensor nodes\n",
    "    5. Graph/ node should be evaluated by running in a session\n",
    "\n",
    "## Datatypes:\n",
    "    1. constant()\n",
    "    2. placeholder()\n",
    "    3. Variable()\n",
    "    \n",
    "## Core training schemas:\n",
    "    1. Optimizers\n",
    "    2. Setup model, then setup loss function, minimize it > Variables get tuned\n",
    "    \n",
    "## tf.contrib.learn\n",
    "    Built-in classes for out-of-box implementation > Eliminates `Setup model` part\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Const_1:0\", shape=(1, 3), dtype=float32)\n",
      "[[  6.  12.  36.]]\n",
      "[[  9.  27.  99.]]\n",
      "[[ 37.  43.  67.]]\n",
      "[[ 0.  1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# constant() and placeholder() example\n",
    "\n",
    "const_node = tf.constant(3.0, dtype=tf.float32)  # A tensor of shape 0\n",
    "print const_node\n",
    "const_node_1 = tf.constant([[3.0, 9.0, 33.0]], dtype=tf.float32)  # A tensor of shape 1\n",
    "print const_node_1\n",
    "\n",
    "# Session concept\n",
    "with tf.Session() as sess:\n",
    "    print sess.run(const_node_1 + const_node)  # Should result in error?\n",
    "    print sess.run(const_node_1 * const_node)  # Should result in error?\n",
    "    print sess.run(const_node_1 + 34)  # Is this boxing int 34 to respective tensorflow Constant int class?\n",
    "    print sess.run(tf.one_hot(indices=[1], depth=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,)\n",
      "[-23. -33. -43. -53.]\n",
      "[-23. -33. -43. -53.]\n",
      "------------------current section---------------------\n",
      "5446.0\n"
     ]
    }
   ],
   "source": [
    "# Variable() example.\n",
    "# constant() will be initialized immediately after declarations and their node values can't change in runtime.\n",
    "# But Variables() values can. They have to be initialized seperately by running a special node\n",
    "\n",
    "# Linear model\n",
    "W = tf.Variable([-10], dtype=tf.float32)  # Shape 1\n",
    "x = tf.placeholder(tf.float32)            # Empty, assertion that says we supply variables later\n",
    "b = tf.Variable([-13], dtype=tf.float32)  # Shape 1\n",
    "b0 = tf.Variable(-13, dtype=tf.float32)   # Shape 0\n",
    "linear_model = W * x + b\n",
    "linear_model0 = W * x + b0\n",
    "# Shapes should be compatable! \n",
    "print W.shape\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print sess.run(linear_model, {x: [1, 2, 3, 4]})\n",
    "    print sess.run(linear_model0, {x: [1, 2, 3, 4]})\n",
    "    \n",
    "    \n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print \"------------------current section---------------------\"\n",
    "# Loss function for linear model\n",
    "y = tf.placeholder(tf.float32)\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y))\n",
    "print sess.run(loss, {x: [1, 2, 3, 4], y: [-1, -2, -3, -4]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 1.00001621], dtype=float32), array([ -4.75694615e-05], dtype=float32), 1.5077433e-09]\n"
     ]
    }
   ],
   "source": [
    "# Training a model\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)  # GD optimizer meat\n",
    "train_node = optimizer.minimize(loss)                # A node with inputs and params linked\n",
    "for _ in range(100):\n",
    "    sess.run(train_node, {x: [1, 2, 3, 4], y: [1, 2, 3, 4]})  # `loss` involves place holders, supply them\n",
    "\n",
    "print sess.run([W, b, loss], {x: [1, 2, 3, 4], y: [1, 2, 3, 4]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "    Use tf.learn to build models out of box with inbuilt types like LinearRegressor, LogisticRegressor etc.. \n",
    "    All these built-in models are sub-classed from tf.learn.Estimator class.\n",
    "    Use it to define your own models which takes advantage of all the abstraction in tf.learn \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "55000 10000 5000\n",
      "(55000, 784)\n",
      "(784,)\n",
      "28.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "print mnist.train.num_examples, mnist.test.num_examples, mnist.validation.num_examples\n",
    "print mnist.train.images.shape\n",
    "\n",
    "batch = mnist.train.next_batch(2)\n",
    "print batch[0][0].shape\n",
    "import math; print math.sqrt(784) # 28 X 28 X 1 Input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training...\n"
     ]
    }
   ],
   "source": [
    "# We will be using a softmax regression as our model which learns 0-9 digits\n",
    "\n",
    "# Inputs \n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "# Trainable vars\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "# Model \n",
    "# y = softmax(Wx + b)\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "# Loss fn.\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "# Minimizing loss fn.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for _ in range(1000):\n",
    "    xs, ys = mnist.train.next_batch(100)\n",
    "    sess.run(train, {x: xs, y_: ys})\n",
    "print \"Finished training...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy : 91.37 percent\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation...\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(\"Prediction accuracy : %.2f percent\" \n",
    "      % (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}) * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Neural Network\n",
    "\n",
    "## Beauty\n",
    "### One line \n",
    "Map a fully lookable image to a vector of features\n",
    "\n",
    "### Summary\n",
    "Input image is say 512 X 512 X 3, it will be convolved, CONVOLUTION LAYER (learns a set of kernal matrices which will pick key features) to new volumes. These new volumes will be sampled-down to lower volumes, POOLING LAYER. These will, after certain depth learn to identify proper high-level features. Then we simply use these features on a FULLY-CONNECTED LAYER to learn and predict classes.\n",
    "\n",
    "### Quick start\n",
    "Gaining momentum on Convolution Neural Network via https://www.youtube.com/watch?v=LxfUGhug-iQ&list=PLwQyV9I_3POsyBPRNUU_ryNfXzgfkiw2p&index=7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying CNN on a MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9.51897144,  10.48738194,  11.74557304,   8.70279503],\n",
       "       [ 10.0640316 ,   8.57131863,   9.68689442,  10.04261208],\n",
       "       [  8.95930099,  10.03546906,   9.84476662,   9.6274662 ],\n",
       "       [  9.76712513,  10.90784836,  10.4563446 ,   9.48399067]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating random numbers to initialize variables.\n",
    "values =  tf.truncated_normal([4,4], stddev=1, mean=10)\n",
    "sess = tf.Session()\n",
    "sess.run(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model description\n",
    "\n",
    " - Input images are of shape (28, 28, 1)\n",
    " - We shall have 2 layers deep network \n",
    " - First layer shall take (28, 28, 1) and return (28, 28, 32); since we plan to use 32 (5, 5) filters \n",
    " - First layer shall use (2, 2) MAX-POOL matrices and recude (28, 28, 32) to (14, 14, 32)\n",
    " - Second layer shall take (14, 14, 32) and apply 64 (5, 5) kernels [Or semantically 2 (5, 5, 32)?]\n",
    " - Second conv layer then shall output (14, 14, 64) tensor which will be passed on to MAX-POOL layer 2\n",
    " - Second max pool layer will again use (2, 2) with stride 2 to reduce its input to (7, 7, 64)\n",
    " - Then we flatten this output to 7 * 7 * 64 vector and input it to a FULLY CONNECTED layer \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 784)\n",
      "(50, 28, 28, 1)\n",
      "step 0, training accuracy 0.06\n",
      "step 100, training accuracy 0.8\n",
      "step 200, training accuracy 0.98\n",
      "step 300, training accuracy 0.96\n",
      "step 400, training accuracy 0.96\n",
      "step 500, training accuracy 0.96\n",
      "step 600, training accuracy 1\n",
      "step 700, training accuracy 0.98\n",
      "step 800, training accuracy 0.98\n",
      "step 900, training accuracy 0.94\n",
      "step 1000, training accuracy 0.92\n",
      "step 1100, training accuracy 0.96\n",
      "step 1200, training accuracy 0.98\n",
      "step 1300, training accuracy 0.98\n",
      "step 1400, training accuracy 0.96\n",
      "step 1500, training accuracy 1\n",
      "step 1600, training accuracy 0.98\n",
      "step 1700, training accuracy 1\n",
      "step 1800, training accuracy 0.98\n",
      "step 1900, training accuracy 0.98\n",
      "step 2000, training accuracy 0.98\n",
      "step 2100, training accuracy 1\n",
      "step 2200, training accuracy 0.98\n",
      "step 2300, training accuracy 1\n",
      "step 2400, training accuracy 1\n",
      "step 2500, training accuracy 0.98\n",
      "step 2600, training accuracy 1\n",
      "step 2700, training accuracy 0.98\n",
      "step 2800, training accuracy 0.98\n",
      "step 2900, training accuracy 0.98\n",
      "step 3000, training accuracy 0.98\n",
      "step 3100, training accuracy 1\n",
      "step 3200, training accuracy 0.98\n",
      "step 3300, training accuracy 1\n",
      "step 3400, training accuracy 1\n",
      "step 3500, training accuracy 1\n",
      "step 3600, training accuracy 0.94\n",
      "step 3700, training accuracy 1\n",
      "step 3800, training accuracy 0.96\n",
      "step 3900, training accuracy 1\n",
      "step 4000, training accuracy 0.98\n",
      "step 4100, training accuracy 0.98\n",
      "step 4200, training accuracy 1\n",
      "step 4300, training accuracy 0.98\n",
      "step 4400, training accuracy 1\n",
      "step 4500, training accuracy 1\n",
      "step 4600, training accuracy 0.98\n",
      "step 4700, training accuracy 0.98\n",
      "step 4800, training accuracy 1\n",
      "step 4900, training accuracy 1\n",
      "step 5000, training accuracy 1\n",
      "step 5100, training accuracy 1\n",
      "step 5200, training accuracy 1\n",
      "step 5300, training accuracy 1\n",
      "step 5400, training accuracy 0.98\n",
      "step 5500, training accuracy 1\n",
      "step 5600, training accuracy 1\n",
      "step 5700, training accuracy 1\n",
      "step 5800, training accuracy 1\n",
      "step 5900, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6100, training accuracy 1\n",
      "step 6200, training accuracy 1\n",
      "step 6300, training accuracy 1\n",
      "step 6400, training accuracy 1\n",
      "step 6500, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6700, training accuracy 1\n",
      "step 6800, training accuracy 1\n",
      "step 6900, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7100, training accuracy 0.98\n",
      "step 7200, training accuracy 1\n",
      "step 7300, training accuracy 0.98\n",
      "step 7400, training accuracy 1\n",
      "step 7500, training accuracy 1\n",
      "step 7600, training accuracy 1\n",
      "step 7700, training accuracy 0.98\n",
      "step 7800, training accuracy 1\n",
      "step 7900, training accuracy 0.98\n",
      "step 8000, training accuracy 0.96\n",
      "step 8100, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8300, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8500, training accuracy 1\n",
      "step 8600, training accuracy 1\n",
      "step 8700, training accuracy 1\n",
      "step 8800, training accuracy 1\n",
      "step 8900, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9100, training accuracy 0.96\n",
      "step 9200, training accuracy 1\n",
      "step 9300, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9500, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9700, training accuracy 1\n",
      "step 9800, training accuracy 0.96\n",
      "step 9900, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 10100, training accuracy 1\n",
      "step 10200, training accuracy 1\n",
      "step 10300, training accuracy 1\n",
      "step 10400, training accuracy 1\n",
      "step 10500, training accuracy 0.96\n",
      "step 10600, training accuracy 1\n",
      "step 10700, training accuracy 1\n",
      "step 10800, training accuracy 1\n",
      "step 10900, training accuracy 0.98\n",
      "step 11000, training accuracy 1\n",
      "step 11100, training accuracy 1\n",
      "step 11200, training accuracy 1\n",
      "step 11300, training accuracy 1\n",
      "step 11400, training accuracy 1\n",
      "step 11500, training accuracy 1\n",
      "step 11600, training accuracy 1\n",
      "step 11700, training accuracy 1\n",
      "step 11800, training accuracy 1\n",
      "step 11900, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 12100, training accuracy 1\n",
      "step 12200, training accuracy 1\n",
      "step 12300, training accuracy 1\n",
      "step 12400, training accuracy 1\n",
      "step 12500, training accuracy 1\n",
      "step 12600, training accuracy 1\n",
      "step 12700, training accuracy 1\n",
      "step 12800, training accuracy 1\n",
      "step 12900, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 13100, training accuracy 1\n",
      "step 13200, training accuracy 1\n",
      "step 13300, training accuracy 1\n",
      "step 13400, training accuracy 1\n",
      "step 13500, training accuracy 1\n",
      "step 13600, training accuracy 1\n",
      "step 13700, training accuracy 1\n",
      "step 13800, training accuracy 1\n",
      "step 13900, training accuracy 0.98\n",
      "step 14000, training accuracy 1\n",
      "step 14100, training accuracy 1\n",
      "step 14200, training accuracy 1\n",
      "step 14300, training accuracy 1\n",
      "step 14400, training accuracy 1\n",
      "step 14500, training accuracy 1\n",
      "step 14600, training accuracy 1\n",
      "step 14700, training accuracy 1\n",
      "step 14800, training accuracy 1\n",
      "step 14900, training accuracy 1\n",
      "step 15000, training accuracy 1\n",
      "step 15100, training accuracy 1\n",
      "step 15200, training accuracy 1\n",
      "step 15300, training accuracy 1\n",
      "step 15400, training accuracy 1\n",
      "step 15500, training accuracy 1\n",
      "step 15600, training accuracy 1\n",
      "step 15700, training accuracy 1\n",
      "step 15800, training accuracy 1\n",
      "step 15900, training accuracy 1\n",
      "step 16000, training accuracy 1\n",
      "step 16100, training accuracy 1\n",
      "step 16200, training accuracy 1\n",
      "step 16300, training accuracy 1\n",
      "step 16400, training accuracy 1\n",
      "step 16500, training accuracy 1\n",
      "step 16600, training accuracy 0.98\n",
      "step 16700, training accuracy 0.98\n",
      "step 16800, training accuracy 1\n",
      "step 16900, training accuracy 1\n",
      "step 17000, training accuracy 1\n",
      "step 17100, training accuracy 1\n",
      "step 17200, training accuracy 1\n",
      "step 17300, training accuracy 1\n",
      "step 17400, training accuracy 1\n",
      "step 17500, training accuracy 1\n",
      "step 17600, training accuracy 1\n",
      "step 17700, training accuracy 1\n",
      "step 17800, training accuracy 1\n",
      "step 17900, training accuracy 1\n",
      "step 18000, training accuracy 1\n",
      "step 18100, training accuracy 1\n",
      "step 18200, training accuracy 1\n",
      "step 18300, training accuracy 1\n",
      "step 18400, training accuracy 1\n",
      "step 18500, training accuracy 1\n",
      "step 18600, training accuracy 1\n",
      "step 18700, training accuracy 0.98\n",
      "step 18800, training accuracy 1\n",
      "step 18900, training accuracy 1\n",
      "step 19000, training accuracy 1\n",
      "step 19100, training accuracy 1\n",
      "step 19200, training accuracy 1\n",
      "step 19300, training accuracy 1\n",
      "step 19400, training accuracy 1\n",
      "step 19500, training accuracy 1\n",
      "step 19600, training accuracy 1\n",
      "step 19700, training accuracy 1\n",
      "step 19800, training accuracy 1\n",
      "step 19900, training accuracy 1\n",
      "###############################################################\n",
      "#                    TRAINING COMPLETE                        #\n",
      "###############################################################\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-463c50cef0ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test accuracy %g'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dsp/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m     \"\"\"\n\u001b[0;32m--> 606\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dsp/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3912\u001b[0m     \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msession\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3914\u001b[0;31m       raise ValueError(\"Cannot evaluate tensor using `eval()`: No default \"\n\u001b[0m\u001b[1;32m   3915\u001b[0m                        \u001b[0;34m\"session is registered. Use `with \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3916\u001b[0m                        \u001b[0;34m\"sess.as_default()` or pass an explicit session to \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Initializing Weights of kernels \n",
    "def get_weights_var(shape):\n",
    "    \n",
    "    weights_matrix = tf.truncated_normal(shape=shape, stddev=0.1)\n",
    "    \n",
    "    return tf.Variable(weights_matrix)\n",
    "\n",
    "# Initializing Biases to use in ReLU output\n",
    "def get_bias_var(shape):\n",
    "    \n",
    "    bias_vect = tf.truncated_normal(shape, stddev=0.1)\n",
    "    \n",
    "    return tf.Variable(bias_vect)\n",
    "\n",
    "# Return a typical Conv layer operation\n",
    "def conv_2d(x, W):\n",
    "    \n",
    "    return tf.nn.conv2d(input=x, filter=W, padding=\"SAME\", strides=[1, 1, 1, 1])\n",
    "\n",
    "# Return a typical Pool layer operation\n",
    "def pool_2x2(x):\n",
    "    \n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "## Start building the graph\n",
    "\n",
    "# First conv layer\n",
    "W1 = get_weights_var([5, 5, 1, 32])\n",
    "b1 = get_bias_var([32])\n",
    "\n",
    "## Reshaping inputs from 784 -> 28x28\n",
    "inputs, labels = mnist.train.next_batch(50)  # We will use labels while training...\n",
    "print inputs.shape\n",
    "x_ = inputs.reshape([-1, 28, 28, 1])\n",
    "print x_.shape\n",
    "\n",
    "## But let's just give x as a place holder as of now...\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "x_images = tf.reshape(x, [-1, 28, 28, 1])\n",
    "conv1 = conv_2d(x_images, W=W1)\n",
    "relu1 = tf.nn.relu(conv1 + b1)\n",
    "pool1 = pool_2x2(relu1)\n",
    "\n",
    "\n",
    "# Second conv layer\n",
    "W2 = get_weights_var([5, 5, 32, 64])\n",
    "b2 = get_bias_var([64])\n",
    "\n",
    "conv2 = conv_2d(x=pool1, W=W2)\n",
    "relu2 = tf.nn.relu(conv2 + b2)\n",
    "pool2 = pool_2x2(relu2)\n",
    "\n",
    "# Fully connected layer \n",
    "\n",
    "## First flatten out output from pool2\n",
    "pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
    "\n",
    "## Now develop vars for fully connected layer\n",
    "fc1_w = get_weights_var([7 * 7 * 64, 1024])\n",
    "fc1_b = get_bias_var([1024])\n",
    "fc1 = tf.nn.relu(tf.matmul(pool2_flat, fc1_w) + fc1_b)\n",
    "\n",
    "# And finally output layer\n",
    "out_w = get_weights_var([1024, 10])\n",
    "out_b = get_bias_var([10])\n",
    "out = tf.matmul(fc1 , out_w) + out_b\n",
    "\n",
    "##############################################################\n",
    "\n",
    "# Uptil here, the graphical model is built \n",
    "# Let's now define metric to esitmate loss and define optimizers to minimize it (TRAINING)\n",
    "y_ = tf.placeholder(dtype=tf.float32)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=out))\n",
    "train = tf.train.AdamOptimizer(1e-04).minimize(loss)\n",
    "\n",
    "\n",
    "##############################################################\n",
    "\n",
    "# Metrics to measure accuracy of trained model\n",
    "correct_prediction = tf.equal(tf.argmax(out, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "##############################################################\n",
    "# Now the actual training...\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={x: batch[0], y_: batch[1]})\n",
    "        print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "    sess.run(train, feed_dict={x: batch[0], y_: batch[1]})\n",
    "    \n",
    "print \"###############################################################\"\n",
    "print \"#                    TRAINING COMPLETE                        #\"\n",
    "print \"###############################################################\"\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 98.93 percent\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy %g percent' % (sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Happy Ending! </center></h2>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
