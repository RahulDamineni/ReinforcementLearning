{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# GridWorld env \n",
    "\n",
    "This is a simple 2D env built by Arthur Juliani which poses full MDP.\n",
    "This gives out images as states, each of which contain an actor, some villans & some heroes.\n",
    "The actor has to reach out heroes for a positive reward, avoid villans to skip the negative reward. \n",
    "I'm not sure if we're paying price for time spent travelling or if we can reach out multiple actors to maximize reward.\n",
    "\n",
    "Let's explore more and figure out more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gridworld import gameEnv\n",
    "\n",
    "# Get env object\n",
    "env = gameEnv(partial=False,size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of actions\n",
    "env.actions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<gridworld.gameOb instance at 0x10f91bf80>,\n",
       " <gridworld.gameOb instance at 0x10f91f3b0>,\n",
       " <gridworld.gameOb instance at 0x10f91f2d8>,\n",
       " <gridworld.gameOb instance at 0x10f91f4d0>,\n",
       " <gridworld.gameOb instance at 0x10f91f0e0>,\n",
       " <gridworld.gameOb instance at 0x10f91f320>,\n",
       " <gridworld.gameOb instance at 0x10f91f290>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All objects in current game\n",
    "env.objects  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('goal', 1, (3, 1), (1, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Object attributes\n",
    "object_ = env.objects[random.randint(0, len(env.objects) - 1)]\n",
    "object_.name, object_.size, (object_.x, object_.y), (object_.channel, object_.intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'fire': 2, 'goal': 4, 'hero': 1})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Objects stats\n",
    "from collections import Counter\n",
    "objects = [object_.name for object_ in env.objects]\n",
    "Counter(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init env\n",
    "state = env.reset()\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_action_random(state):\n",
    "    '''\n",
    "    Returns randomly one action from [0, 1, 2, 3]\n",
    "    '''\n",
    "    return random.choice([0, 1, 2, 3])\n",
    "\n",
    "get_action_random(state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for choosing action: 2 is 1.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 3 is 1.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 0 is 1.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 3 is 1.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 3 is -1.0\n",
      "Reward for choosing action: 0 is 1.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 1 is 1.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Game ended after 101 units of time\n"
     ]
    }
   ],
   "source": [
    "# Play a game to get an idea on the dynamics\n",
    "state = env.reset()\n",
    "done = False\n",
    "counter = 0\n",
    "\n",
    "while not done:\n",
    "    action = get_action_random(state)\n",
    "    state, reward, done = env.step(action)\n",
    "    \n",
    "\n",
    "    print(\"Reward for choosing action: {} is {}\".format(\n",
    "            action, reward\n",
    "    ))\n",
    "\n",
    "    counter += 1\n",
    "    \n",
    "    if counter > 100:\n",
    "        break\n",
    "\n",
    "print(\"Game ended after {} units of time\".format(\n",
    "        counter\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Observations\n",
    "* Looks like, every time step, `hero` has to move in one of known four directions. \n",
    "* If he passes through `fire`, a -1 but passes through a `goal`, a +1, other-wise a zero.\n",
    "* There is no `end` condition for this env, so the goal must be to maximise total reward.\n",
    "\n",
    "* Another thing, this env is dynamically populated. Meaning, the objects are randomly placed for each episode, so roting won't help the env. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Deep Q Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture (Vanilla)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import (\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from gridworld import gameEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Architecture parameters\n",
    "kernel_size = (9, 9)\n",
    "pool_stride = (2, 2)\n",
    "conv_activation = \"tanh\"\n",
    "dense_activation = \"relu\"\n",
    "output_activation = \"linear\"\n",
    "lr = 0.1\n",
    "optimiser = \"adam\" # (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnvDQNAgent(object):\n",
    "    '''\n",
    "    An agent which exposes `learn` and `demo` methods for learning GridEnv env \n",
    "    and running a demo what it has learnt.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, exploration_prob=1.0, exploration_prob_min=0.1, max_episodes=10000, max_episode_length=500, batch_size=20):\n",
    "        '''\n",
    "        Tunable parameters (partial list)\n",
    "        Apart from these, there are optimiser types, learning rates, decay rates; \n",
    "        model architecture parameters; reward disgestion mechanism as variables.\n",
    "        '''\n",
    "        self.exploration_prob = exploration_prob\n",
    "        self.exploration_prob_min = exploration_prob_min\n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_episode_length = max_episode_length\n",
    "        self.batch_size = batch_size\n",
    "        self.agent = self.build_DQN(input_dims=(84, 84, 3), number_of_actions=4)\n",
    "        self.env = gameEnv(partial=False,size=5)\n",
    "        self.pick_policy = \"!random\"  # Set this to any other value to train agent with most recent few experiences P .  \n",
    "        self.experiences = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def build_DQN(self, input_dims, number_of_actions):\n",
    "        '''\n",
    "        A Deep network which takes state as input & outputs action weights.\n",
    "        Essentially this learns the Q table for solving CartPole, hence the name Deep-Q-Network.\n",
    "        '''\n",
    "        self.input_dims = input_dims\n",
    "        input_ = Input(shape=input_dims)\n",
    "        \n",
    "        c1 = Conv2D(activation=conv_activation, filters=32, kernel_size=kernel_size, padding=\"SAME\")(input_)\n",
    "        c1 = MaxPooling2D(strides=pool_stride, pool_size=pool_stride)(c1)\n",
    "        \n",
    "        c2 = Conv2D(activation=conv_activation, filters=128, kernel_size=kernel_size, padding=\"SAME\")(c1)\n",
    "        c2 = MaxPooling2D(strides=pool_stride, pool_size=pool_stride)(c2)\n",
    "        \n",
    "        c3 = Conv2D(activation=conv_activation, filters=256, kernel_size=kernel_size, padding=\"SAME\")(c2)\n",
    "        c3 = MaxPooling2D(strides=pool_stride, pool_size=pool_stride)(c3)\n",
    "        \n",
    "        flattened = Flatten()(c3)\n",
    "        \n",
    "        d1 = Dense(1024, activation=dense_activation)(flattened)\n",
    "        d2 = Dense(512, activation=dense_activation)(d1)\n",
    "        output_ = Dense(number_of_actions, activation=output_activation)(d2)\n",
    "        \n",
    "        agent_model = Model(inputs=[input_], outputs=[output_])\n",
    "        \n",
    "        agent_model.compile(\n",
    "            optimizer=optimiser,\n",
    "            loss=\"mse\",\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "        \n",
    "        print(agent_model.summary())\n",
    "        \n",
    "        return agent_model\n",
    "    \n",
    "    \n",
    "    def get_greedy_action(self, state):\n",
    "        '''\n",
    "        Given a state, estimates action probability distribution using DQN (agent).\n",
    "        Then it would return most valuable action with a probability of `exploration_prob`\n",
    "        '''\n",
    "        if random.random() <= self.exploration_prob:\n",
    "            action = random.choice([0, 1, 2, 3])\n",
    "        else:\n",
    "            actions_pd = self.agent.predict(state.reshape(self.reshape_dims))\n",
    "            action = np.argmax(actions_pd)\n",
    "\n",
    "        return action\n",
    "        \n",
    "    def get_training_data(self, pick_policy):\n",
    "        '''\n",
    "        When invoked, returns at most self.batch_size number of experiences \n",
    "        from self.experiences\n",
    "        '''       \n",
    "        mini_batch_size = min(self.batch_size, len(self.experiences))\n",
    "        if pick_policy == \"random\":\n",
    "            # Pick randomly \n",
    "            mini_batch = random.sample(\n",
    "                population=self.experiences,\n",
    "                k=mini_batch_size\n",
    "            )\n",
    "        else:\n",
    "            # Pick last few\n",
    "            mini_batch = self.experiences[-mini_batch_size:]\n",
    "        \n",
    "        trainX, trainY = [], []\n",
    "        for x, y in mini_batch:\n",
    "            trainX.append(x)\n",
    "            trainY.append(y)\n",
    "\n",
    "        return np.array(trainX), np.array(trainY)\n",
    "    \n",
    "    def decay_exploration_prob(self):\n",
    "        '''\n",
    "        Slowly decays the exploration probabilty with runtime\n",
    "        '''\n",
    "        self.exploration_prob -= 1. / self.max_episodes\n",
    "        self.exploration_prob = max(self.exploration_prob, self.exploration_prob_min)\n",
    "    \n",
    "    def learn(self):\n",
    "        '''\n",
    "        Initialises env, plays & updates DQN to the optimal Q\n",
    "        '''   \n",
    "        # Play a maximum of `max_episodes` number of games\n",
    "        for game in range(1, self.max_episodes + 1):\n",
    "            \n",
    "            self.experiences = []\n",
    "            current_state = self.env.reset()\n",
    "            accum_reward = 0\n",
    "            \n",
    "            # Stock experiences into buffer of length max number of time units a game should sustain\n",
    "            for time_unit in range(self.max_episode_length):\n",
    "                \n",
    "                # Take an action (exploratorily) and see how it rewards now \n",
    "                action = self.get_greedy_action(state=current_state)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                \n",
    "                # Now update the action_pd responsible for above action by including \n",
    "                # current reward and value we'd achieve by following optimal policy \n",
    "                # from now on, given we took above action\n",
    "                self.reshape_dims = tuple([-1]) + self.input_dims\n",
    "                target_action_pd = self.agent.predict(current_state.reshape(self.reshape_dims))[0]\n",
    "                target_action_pd[action] = reward + 0.99 * np.max(self.agent.predict(next_state.reshape(self.reshape_dims))[0])\n",
    "                if done is True:  # In this env, done is never true.\n",
    "                    target_action_pd[action] = -1\n",
    "                \n",
    "                # Stock the experiences\n",
    "                self.experiences.append([current_state.tolist(), target_action_pd.tolist()])\n",
    "                current_state = next_state\n",
    "                accum_reward += reward\n",
    "                \n",
    "                # Check if it's time to update policy (train DQN) \n",
    "                if time_unit % self.batch_size == 0 or done is True:\n",
    "                    trainX, trainY = self.get_training_data(pick_policy=self.pick_policy)\n",
    "                    self.agent.train_on_batch(trainX, trainY)\n",
    "                \n",
    "                # Exit if game's over\n",
    "                if done is True:  # In this env, done is never true.\n",
    "                    break\n",
    "            \n",
    "            self.rewards.append(accum_reward)\n",
    "            self.decay_exploration_prob()\n",
    "            \n",
    "            stat_update_freq = 2\n",
    "            if game % stat_update_freq == 0:\n",
    "                avg_reward = np.array(self.rewards[-100:]).mean()\n",
    "                print(\"Avg. reward over last {0:d} is {1:3.2f}. Last played game#{2:d}\".format(\n",
    "                    stat_update_freq, avg_reward, game\n",
    "                ))\n",
    "                if avg_reward > 195.0:\n",
    "                    print(\"Done solving... \")\n",
    "                    print(\"\"\"(CartPole-v0 defines \"solving\" as getting average reward \n",
    "                    of 195.0 over 100 consecutive trials.)\"\"\")\n",
    "                    break\n",
    "            \n",
    "            \n",
    "    def demo(self):\n",
    "        '''\n",
    "        Run this to render the performance of trained model.\n",
    "        Be sure to first train the model.\n",
    "        '''\n",
    "        total_reward = 0\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.argmax(self.agent.predict(state.reshape(-1, 4))[0])\n",
    "            state, reward, done = self.env.step(action)\n",
    "            self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RL Hyperparameters\n",
    "exploration_prob = 1.0\n",
    "exploration_prob_min = 0.1\n",
    "max_episodes = 10000\n",
    "max_episode_length = 50\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 84, 84, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 84, 84, 32)        7808      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 42, 42, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 42, 42, 128)       331904    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 21, 21, 256)       2654464   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 1024)              26215424  \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 29,736,452\n",
      "Trainable params: 29,736,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Instantiate GridEnvDQNAgent object.\n",
    "# This object should be trained afresh everytime a notebook is restarted\n",
    "# Consider saving weights of trained agent to make demo work out of box\n",
    "DQNAgent = GridEnvDQNAgent(\n",
    "    max_episodes=max_episodes,\n",
    "    exploration_prob_min=exploration_prob_min,\n",
    "    batch_size=batch_size,\n",
    "    max_episode_length=max_episode_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. reward over last 2 is 3.00. Last played game#2\n",
      "Avg. reward over last 2 is 2.50. Last played game#4\n",
      "Avg. reward over last 2 is 2.83. Last played game#6\n",
      "Avg. reward over last 2 is 2.88. Last played game#8\n",
      "Avg. reward over last 2 is 2.10. Last played game#10\n",
      "Avg. reward over last 2 is 2.58. Last played game#12\n",
      "Avg. reward over last 2 is 2.07. Last played game#14\n",
      "Avg. reward over last 2 is 2.00. Last played game#16\n",
      "Avg. reward over last 2 is 2.17. Last played game#18\n",
      "Avg. reward over last 2 is 1.75. Last played game#20\n",
      "Avg. reward over last 2 is 1.73. Last played game#22\n",
      "Avg. reward over last 2 is 1.79. Last played game#24\n",
      "Avg. reward over last 2 is 1.73. Last played game#26\n",
      "Avg. reward over last 2 is 1.82. Last played game#28\n",
      "Avg. reward over last 2 is 1.73. Last played game#30\n",
      "Avg. reward over last 2 is 1.94. Last played game#32\n",
      "Avg. reward over last 2 is 1.79. Last played game#34\n",
      "Avg. reward over last 2 is 1.86. Last played game#36\n",
      "Avg. reward over last 2 is 1.82. Last played game#38\n",
      "Avg. reward over last 2 is 1.82. Last played game#40\n",
      "Avg. reward over last 2 is 1.93. Last played game#42\n",
      "Avg. reward over last 2 is 1.95. Last played game#44\n",
      "Avg. reward over last 2 is 2.11. Last played game#46\n",
      "Avg. reward over last 2 is 2.08. Last played game#48\n",
      "Avg. reward over last 2 is 2.18. Last played game#50\n",
      "Avg. reward over last 2 is 2.19. Last played game#52\n",
      "Avg. reward over last 2 is 2.22. Last played game#54\n",
      "Avg. reward over last 2 is 2.23. Last played game#56\n",
      "Avg. reward over last 2 is 2.17. Last played game#58\n",
      "Avg. reward over last 2 is 2.17. Last played game#60\n",
      "Avg. reward over last 2 is 2.15. Last played game#62\n",
      "Avg. reward over last 2 is 2.02. Last played game#64\n",
      "Avg. reward over last 2 is 1.98. Last played game#66\n",
      "Avg. reward over last 2 is 2.01. Last played game#68\n",
      "Avg. reward over last 2 is 2.00. Last played game#70\n",
      "Avg. reward over last 2 is 2.08. Last played game#72\n",
      "Avg. reward over last 2 is 2.12. Last played game#74\n",
      "Avg. reward over last 2 is 2.05. Last played game#76\n",
      "Avg. reward over last 2 is 2.10. Last played game#78\n",
      "Avg. reward over last 2 is 2.19. Last played game#80\n",
      "Avg. reward over last 2 is 2.21. Last played game#82\n",
      "Avg. reward over last 2 is 2.19. Last played game#84\n",
      "Avg. reward over last 2 is 2.20. Last played game#86\n",
      "Avg. reward over last 2 is 2.10. Last played game#88\n",
      "Avg. reward over last 2 is 2.14. Last played game#90\n",
      "Avg. reward over last 2 is 2.14. Last played game#92\n",
      "Avg. reward over last 2 is 2.16. Last played game#94\n",
      "Avg. reward over last 2 is 2.11. Last played game#96\n",
      "Avg. reward over last 2 is 2.05. Last played game#98\n",
      "Avg. reward over last 2 is 2.03. Last played game#100\n",
      "Avg. reward over last 2 is 2.10. Last played game#102\n",
      "Avg. reward over last 2 is 2.11. Last played game#104\n",
      "Avg. reward over last 2 is 2.18. Last played game#106\n",
      "Avg. reward over last 2 is 2.14. Last played game#108\n",
      "Avg. reward over last 2 is 2.14. Last played game#110\n",
      "Avg. reward over last 2 is 2.10. Last played game#112\n",
      "Avg. reward over last 2 is 2.13. Last played game#114\n",
      "Avg. reward over last 2 is 2.21. Last played game#116\n",
      "Avg. reward over last 2 is 2.15. Last played game#118\n",
      "Avg. reward over last 2 is 2.25. Last played game#120\n",
      "Avg. reward over last 2 is 2.26. Last played game#122\n",
      "Avg. reward over last 2 is 2.23. Last played game#124\n",
      "Avg. reward over last 2 is 2.23. Last played game#126\n",
      "Avg. reward over last 2 is 2.21. Last played game#128\n",
      "Avg. reward over last 2 is 2.17. Last played game#130\n",
      "Avg. reward over last 2 is 2.10. Last played game#132\n",
      "Avg. reward over last 2 is 2.07. Last played game#134\n",
      "Avg. reward over last 2 is 2.03. Last played game#136\n",
      "Avg. reward over last 2 is 2.07. Last played game#138\n",
      "Avg. reward over last 2 is 2.01. Last played game#140\n",
      "Avg. reward over last 2 is 1.99. Last played game#142\n",
      "Avg. reward over last 2 is 1.94. Last played game#144\n",
      "Avg. reward over last 2 is 1.91. Last played game#146\n",
      "Avg. reward over last 2 is 1.90. Last played game#148\n",
      "Avg. reward over last 2 is 1.89. Last played game#150\n",
      "Avg. reward over last 2 is 1.85. Last played game#152\n",
      "Avg. reward over last 2 is 1.78. Last played game#154\n",
      "Avg. reward over last 2 is 1.77. Last played game#156\n",
      "Avg. reward over last 2 is 1.74. Last played game#158\n",
      "Avg. reward over last 2 is 1.81. Last played game#160\n",
      "Avg. reward over last 2 is 1.85. Last played game#162\n",
      "Avg. reward over last 2 is 1.92. Last played game#164\n",
      "Avg. reward over last 2 is 1.90. Last played game#166\n",
      "Avg. reward over last 2 is 1.89. Last played game#168\n",
      "Avg. reward over last 2 is 1.88. Last played game#170\n",
      "Avg. reward over last 2 is 1.81. Last played game#172\n",
      "Avg. reward over last 2 is 1.76. Last played game#174\n"
     ]
    }
   ],
   "source": [
    "# Explore, sync reward and learn\n",
    "DQNAgent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
