{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# GridWorld env \n",
    "\n",
    "This is a simple 2D env built by Arthur Juliani which poses full MDP.\n",
    "This gives out images as states, each of which contain an actor, some villans & some heroes.\n",
    "The actor has to reach out heroes for a positive reward, avoid villans to skip the negative reward. \n",
    "I'm not sure if we're paying price for time spent travelling or if we can reach out multiple actors to maximize reward.\n",
    "\n",
    "Let's explore more and figure out more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gridworld import gameEnv\n",
    "\n",
    "# Get env object\n",
    "env = gameEnv(partial=False,size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of actions\n",
    "env.actions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<gridworld.gameOb instance at 0x10f91bf80>,\n",
       " <gridworld.gameOb instance at 0x10f91f3b0>,\n",
       " <gridworld.gameOb instance at 0x10f91f2d8>,\n",
       " <gridworld.gameOb instance at 0x10f91f4d0>,\n",
       " <gridworld.gameOb instance at 0x10f91f0e0>,\n",
       " <gridworld.gameOb instance at 0x10f91f320>,\n",
       " <gridworld.gameOb instance at 0x10f91f290>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All objects in current game\n",
    "env.objects  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('goal', 1, (3, 1), (1, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Object attributes\n",
    "object_ = env.objects[random.randint(0, len(env.objects) - 1)]\n",
    "object_.name, object_.size, (object_.x, object_.y), (object_.channel, object_.intensity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'fire': 2, 'goal': 4, 'hero': 1})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Objects stats\n",
    "from collections import Counter\n",
    "objects = [object_.name for object_ in env.objects]\n",
    "Counter(objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init env\n",
    "state = env.reset()\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_action_random(state):\n",
    "    '''\n",
    "    Returns randomly one action from [0, 1, 2, 3]\n",
    "    '''\n",
    "    return random.choice([0, 1, 2, 3])\n",
    "\n",
    "get_action_random(state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward for choosing action: 2 is 1.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 3 is 1.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 0 is 1.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 3 is 1.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 3 is -1.0\n",
      "Reward for choosing action: 0 is 1.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 1 is 1.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 0 is 0.0\n",
      "Reward for choosing action: 2 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Reward for choosing action: 3 is 0.0\n",
      "Reward for choosing action: 1 is 0.0\n",
      "Game ended after 101 units of time\n"
     ]
    }
   ],
   "source": [
    "# Play a game to get an idea on the dynamics\n",
    "state = env.reset()\n",
    "done = False\n",
    "counter = 0\n",
    "\n",
    "while not done:\n",
    "    action = get_action_random(state)\n",
    "    state, reward, done = env.step(action)\n",
    "    \n",
    "\n",
    "    print(\"Reward for choosing action: {} is {}\".format(\n",
    "            action, reward\n",
    "    ))\n",
    "\n",
    "    counter += 1\n",
    "    \n",
    "    if counter > 100:\n",
    "        break\n",
    "\n",
    "print(\"Game ended after {} units of time\".format(\n",
    "        counter\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Observations\n",
    "* Looks like, every time step, `hero` has to move in one of known four directions. \n",
    "* If he passes through `fire`, a -1 but passes through a `goal`, a +1, other-wise a zero.\n",
    "* There is no `end` condition for this env, so the goal must be to maximise total reward.\n",
    "\n",
    "* Another thing, this env is dynamically populated. Meaning, the objects are randomly placed for each episode, so roting won't help the env. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Deep Q Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture (Vanilla)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import (\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    MaxPooling2D,\n",
    "    Flatten,\n",
    "    Input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from gridworld import gameEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Architecture parameters\n",
    "kernel_size = (9, 9)\n",
    "pool_stride = (2, 2)\n",
    "conv_activation = \"tanh\"\n",
    "dense_activation = \"relu\"\n",
    "output_activation = \"linear\"\n",
    "lr = 0.1\n",
    "optimiser = \"adam\" # (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GridEnvDQNAgent(object):\n",
    "    '''\n",
    "    An agent which exposes `learn` and `demo` methods for learning GridEnv env \n",
    "    and running a demo what it has learnt.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, exploration_prob=1.0, exploration_prob_min=0.1, max_episodes=10000, max_episode_length=500, batch_size=20):\n",
    "        '''\n",
    "        Tunable parameters (partial list)\n",
    "        Apart from these, there are optimiser types, learning rates, decay rates; \n",
    "        model architecture parameters; reward disgestion mechanism as variables.\n",
    "        '''\n",
    "        self.exploration_prob = exploration_prob\n",
    "        self.exploration_prob_min = exploration_prob_min\n",
    "        self.max_episodes = max_episodes\n",
    "        self.max_episode_length = max_episode_length\n",
    "        self.batch_size = batch_size\n",
    "        self.agent = self.build_DQN(input_dims=(84, 84, 3), number_of_actions=4)\n",
    "        self.env = gameEnv(partial=False,size=5)\n",
    "        self.pick_policy = \"!random\"  # Set this to any other value to train agent with most recent few experiences P .  \n",
    "        self.experiences = []\n",
    "        self.rewards = []\n",
    "        \n",
    "    def build_DQN(self, input_dims, number_of_actions):\n",
    "        '''\n",
    "        A Deep network which takes state as input & outputs action weights.\n",
    "        Essentially this learns the Q table for solving CartPole, hence the name Deep-Q-Network.\n",
    "        '''\n",
    "        self.input_dims = input_dims\n",
    "        input_ = Input(shape=input_dims)\n",
    "        \n",
    "        c1 = Conv2D(activation=conv_activation, filters=32, kernel_size=kernel_size, padding=\"SAME\")(input_)\n",
    "        c1 = MaxPooling2D(strides=pool_stride, pool_size=pool_stride)(c1)\n",
    "        \n",
    "        c2 = Conv2D(activation=conv_activation, filters=128, kernel_size=kernel_size, padding=\"SAME\")(c1)\n",
    "        c2 = MaxPooling2D(strides=pool_stride, pool_size=pool_stride)(c2)\n",
    "        \n",
    "        c3 = Conv2D(activation=conv_activation, filters=256, kernel_size=kernel_size, padding=\"SAME\")(c2)\n",
    "        c3 = MaxPooling2D(strides=pool_stride, pool_size=pool_stride)(c3)\n",
    "        c3 = MaxPooling2D(strides=pool_stride, pool_size=pool_stride)(c3)\n",
    "        \n",
    "        flattened = Flatten()(c3)\n",
    "        \n",
    "        d1 = Dense(1024, activation=dense_activation)(flattened)\n",
    "        d2 = Dense(512, activation=dense_activation)(d1)\n",
    "        output_ = Dense(number_of_actions, activation=output_activation)(d2)\n",
    "        \n",
    "        agent_model = Model(inputs=[input_], outputs=[output_])\n",
    "        \n",
    "        agent_model.compile(\n",
    "            optimizer=optimiser,\n",
    "            loss=\"mse\",\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "        \n",
    "        print(agent_model.summary())\n",
    "        \n",
    "        return agent_model\n",
    "    \n",
    "    \n",
    "    def get_greedy_action(self, state):\n",
    "        '''\n",
    "        Given a state, estimates action probability distribution using DQN (agent).\n",
    "        Then it would return most valuable action with a probability of `exploration_prob`\n",
    "        '''\n",
    "        if random.random() <= self.exploration_prob:\n",
    "            action = random.choice([0, 1, 2, 3])\n",
    "        else:\n",
    "            actions_pd = self.agent.predict(state.reshape(self.reshape_dims))\n",
    "            action = np.argmax(actions_pd)\n",
    "\n",
    "        return action\n",
    "        \n",
    "    def get_training_data(self, pick_policy):\n",
    "        '''\n",
    "        When invoked, returns at most self.batch_size number of experiences \n",
    "        from self.experiences\n",
    "        '''       \n",
    "        mini_batch_size = min(self.batch_size, len(self.experiences))\n",
    "        if pick_policy == \"random\":\n",
    "            # Pick randomly \n",
    "            mini_batch = random.sample(\n",
    "                population=self.experiences,\n",
    "                k=mini_batch_size\n",
    "            )\n",
    "        else:\n",
    "            # Pick last few\n",
    "            mini_batch = self.experiences[-mini_batch_size:]\n",
    "        \n",
    "        trainX, trainY = [], []\n",
    "        for x, y in mini_batch:\n",
    "            trainX.append(x)\n",
    "            trainY.append(y)\n",
    "\n",
    "        return np.array(trainX), np.array(trainY)\n",
    "    \n",
    "    def decay_exploration_prob(self):\n",
    "        '''\n",
    "        Slowly decays the exploration probabilty with runtime\n",
    "        '''\n",
    "        self.exploration_prob -= 1. / self.max_episodes\n",
    "        self.exploration_prob = max(self.exploration_prob, self.exploration_prob_min)\n",
    "    \n",
    "    def learn(self):\n",
    "        '''\n",
    "        Initialises env, plays & updates DQN to the optimal Q\n",
    "        '''   \n",
    "        # Play a maximum of `max_episodes` number of games\n",
    "        for game in range(1, self.max_episodes + 1):\n",
    "            \n",
    "            self.experiences = []\n",
    "            current_state = self.env.reset()\n",
    "            accum_reward = 0\n",
    "            \n",
    "            # Stock experiences into buffer of length max number of time units a game should sustain\n",
    "            for time_unit in range(self.max_episode_length):\n",
    "                \n",
    "                # Take an action (exploratorily) and see how it rewards now \n",
    "                action = self.get_greedy_action(state=current_state)\n",
    "                next_state, reward, done = self.env.step(action)\n",
    "                \n",
    "                # Now update the action_pd responsible for above action by including \n",
    "                # current reward and value we'd achieve by following optimal policy \n",
    "                # from now on, given we took above action\n",
    "                self.reshape_dims = tuple([-1]) + self.input_dims\n",
    "                target_action_pd = self.agent.predict(current_state.reshape(self.reshape_dims))[0]\n",
    "                target_action_pd[action] = reward + 0.99 * np.max(self.agent.predict(next_state.reshape(self.reshape_dims))[0])\n",
    "                if done is True:  # In this env, done is never true.\n",
    "                    target_action_pd[action] = -1\n",
    "                \n",
    "                # Stock the experiences\n",
    "                self.experiences.append([current_state.tolist(), target_action_pd.tolist()])\n",
    "                current_state = next_state\n",
    "                accum_reward += reward\n",
    "                \n",
    "                # Check if it's time to update policy (train DQN) \n",
    "                if time_unit % self.batch_size == 0 or done is True:\n",
    "                    trainX, trainY = self.get_training_data(pick_policy=self.pick_policy)\n",
    "                    self.agent.train_on_batch(trainX, trainY)\n",
    "                \n",
    "                # Exit if game's over\n",
    "                if done is True:  # In this env, done is never true.\n",
    "                    break\n",
    "            \n",
    "            self.rewards.append(accum_reward)\n",
    "            self.decay_exploration_prob()\n",
    "            \n",
    "            stat_update_freq = 8\n",
    "            if game % stat_update_freq == 0:\n",
    "                avg_reward = np.array(self.rewards[-100:]).mean()\n",
    "                print(\"Avg. reward over last {0:d} is {1:3.2f}. Last played game#{2:d}\".format(\n",
    "                    stat_update_freq, avg_reward, game\n",
    "                ))\n",
    "                if avg_reward > 195.0:\n",
    "                    print(\"Done solving... \")\n",
    "                    print(\"\"\"(CartPole-v0 defines \"solving\" as getting average reward \n",
    "                    of 195.0 over 100 consecutive trials.)\"\"\")\n",
    "                    break\n",
    "            \n",
    "            \n",
    "    def demo(self):\n",
    "        '''\n",
    "        Run this to render the performance of trained model.\n",
    "        Be sure to first train the model.\n",
    "        '''\n",
    "        total_reward = 0\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            action = np.argmax(self.agent.predict(state.reshape(-1, 4))[0])\n",
    "            state, reward, done = self.env.step(action)\n",
    "            self.env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RL Hyperparameters\n",
    "exploration_prob = 1.0\n",
    "exploration_prob_min = 0.1\n",
    "max_episodes = 10000\n",
    "max_episode_length = 50\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 84, 84, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 84, 84, 32)        7808      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 42, 42, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 42, 42, 128)       331904    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 21, 21, 256)       2654464   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1024)              6554624   \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 10,075,652\n",
      "Trainable params: 10,075,652\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Instantiate GridEnvDQNAgent object.\n",
    "# This object should be trained afresh everytime a notebook is restarted\n",
    "# Consider saving weights of trained agent to make demo work out of box\n",
    "DQNAgent = GridEnvDQNAgent(\n",
    "    max_episodes=max_episodes,\n",
    "    exploration_prob_min=exploration_prob_min,\n",
    "    batch_size=batch_size,\n",
    "    max_episode_length=max_episode_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. reward over last 8 is 2.38. Last played game#8\n",
      "Avg. reward over last 8 is 2.75. Last played game#16\n",
      "Avg. reward over last 8 is 2.21. Last played game#24\n",
      "Avg. reward over last 8 is 1.91. Last played game#32\n",
      "Avg. reward over last 8 is 1.80. Last played game#40\n",
      "Avg. reward over last 8 is 1.88. Last played game#48\n",
      "Avg. reward over last 8 is 1.88. Last played game#56\n",
      "Avg. reward over last 8 is 1.95. Last played game#64\n",
      "Avg. reward over last 8 is 2.06. Last played game#72\n",
      "Avg. reward over last 8 is 1.98. Last played game#80\n",
      "Avg. reward over last 8 is 1.93. Last played game#88\n",
      "Avg. reward over last 8 is 1.88. Last played game#96\n",
      "Avg. reward over last 8 is 1.74. Last played game#104\n",
      "Avg. reward over last 8 is 1.74. Last played game#112\n",
      "Avg. reward over last 8 is 1.71. Last played game#120\n",
      "Avg. reward over last 8 is 1.93. Last played game#128\n",
      "Avg. reward over last 8 is 1.98. Last played game#136\n",
      "Avg. reward over last 8 is 2.08. Last played game#144\n",
      "Avg. reward over last 8 is 2.11. Last played game#152\n",
      "Avg. reward over last 8 is 2.12. Last played game#160\n",
      "Avg. reward over last 8 is 2.21. Last played game#168\n",
      "Avg. reward over last 8 is 1.99. Last played game#176\n",
      "Avg. reward over last 8 is 2.01. Last played game#184\n",
      "Avg. reward over last 8 is 2.11. Last played game#192\n",
      "Avg. reward over last 8 is 2.29. Last played game#200\n",
      "Avg. reward over last 8 is 2.06. Last played game#208\n",
      "Avg. reward over last 8 is 2.12. Last played game#216\n",
      "Avg. reward over last 8 is 2.02. Last played game#224\n",
      "Avg. reward over last 8 is 1.92. Last played game#232\n",
      "Avg. reward over last 8 is 1.81. Last played game#240\n",
      "Avg. reward over last 8 is 1.73. Last played game#248\n",
      "Avg. reward over last 8 is 1.77. Last played game#256\n",
      "Avg. reward over last 8 is 1.83. Last played game#264\n",
      "Avg. reward over last 8 is 1.93. Last played game#272\n",
      "Avg. reward over last 8 is 1.93. Last played game#280\n",
      "Avg. reward over last 8 is 1.76. Last played game#288\n",
      "Avg. reward over last 8 is 1.78. Last played game#296\n",
      "Avg. reward over last 8 is 1.93. Last played game#304\n",
      "Avg. reward over last 8 is 1.91. Last played game#312\n",
      "Avg. reward over last 8 is 1.91. Last played game#320\n",
      "Avg. reward over last 8 is 1.97. Last played game#328\n",
      "Avg. reward over last 8 is 1.90. Last played game#336\n",
      "Avg. reward over last 8 is 2.19. Last played game#344\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1219ad77b4bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Explore, sync reward and learn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-75e6bbe52cf8>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mtarget_action_pd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0mtarget_action_pd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.99\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# In this env, done is never true.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                     \u001b[0mtarget_action_pd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dsp/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1711\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1713\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/Users/dsp/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1267\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m                     \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1269\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1270\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dsp/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dsp/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dsp/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dsp/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dsp/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dsp/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Explore, sync reward and learn\n",
    "DQNAgent.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
