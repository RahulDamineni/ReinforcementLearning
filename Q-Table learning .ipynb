{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Fundamentals of Q learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we start, let's learn about gym\n",
    "\n",
    "    * Gym is an OpenAI platform which provides easy access to various environments. \n",
    "    * Environment (of a game) will have attributes such as `action_space`, `observation_space` & `reward_range`\n",
    "    * We use `step()` method to advance environment when a particular action is chosen\n",
    "    * `step()` returns four outputs; observation, reward, done, info\n",
    "        > Observation is `Discrete`/ `Box` object which gives info about what state env is in currently \n",
    "        > Reward is what we achieved after action\n",
    "        > Done will be true if the env reaches its end state\n",
    "        > Info contains metadata\n",
    "        \n",
    "Read more here: https://gym.openai.com/docs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now a brief about RL \n",
    "\n",
    "Reinforced Learning is all about learning what chain of actions to follow to go through a certain chain of states in order to reach a final state in a way that maximizes our **total** reward\n",
    "\n",
    "### Terminology\n",
    "1. Environment\n",
    "2. Agent\n",
    "3. State\n",
    "4. Observation\n",
    "5. Action\n",
    "6. Reward\n",
    "7. Environment modelling by an Agent\n",
    "8. Policy of an Agent\n",
    "9. Value function of an Agent\n",
    "    * Value function for a fixed action ($\\pi$) for each state $(S)$ : $Q_{\\pi}(S_i)$\n",
    "    * Value function for variable actions for each state $(S)$ : $Q(a_i, S_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The whole drama is for learning $Q(a_i, S_i)$\n",
    "\n",
    "The simplest form of representing this function is using a table. \n",
    "A table of dimension $(#states, #actions)$. Each cell in the table denotes the value of being in a state $s$ and taking an action $a$ in $s$\n",
    "\n",
    "## Frozen lake env in Gym\n",
    "To establish and learn that kind of Q-table, Frozen lake env in Gym would be best fit.\n",
    "This env is 4x4 grid, each cell can be a stable block, a hole or a goal position. \n",
    "Reward to be in any cell except goal cell is zero. Goal cell carries a reward of one.\n",
    "\n",
    "Now possible states in this env is one of the cells, so, 16.\n",
    "Similarly possible actions (left, right, top, bottom) are 4.\n",
    "Let's dive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-21 01:58:51,712] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#states = 16\n",
      "#actions = 4\n"
     ]
    }
   ],
   "source": [
    "print \"#states =\", env.observation_space.n\n",
    "print \"#actions =\", env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward @ episode 100 = 0.00\n",
      "Reward @ episode 200 = 0.02\n",
      "Reward @ episode 300 = 0.04\n",
      "Reward @ episode 400 = 0.06\n",
      "Reward @ episode 500 = 0.09\n",
      "Reward @ episode 600 = 0.11\n",
      "Reward @ episode 700 = 0.14\n",
      "Reward @ episode 800 = 0.17\n",
      "Reward @ episode 900 = 0.21\n",
      "Reward @ episode 1000 = 0.24\n",
      "Reward @ episode 1100 = 0.28\n",
      "Reward @ episode 1200 = 0.31\n",
      "Reward @ episode 1300 = 0.34\n",
      "Reward @ episode 1400 = 0.37\n",
      "Reward @ episode 1500 = 0.41\n",
      "Reward @ episode 1600 = 0.45\n",
      "Reward @ episode 1700 = 0.49\n",
      "Reward @ episode 1800 = 0.52\n",
      "Reward @ episode 1900 = 0.56\n",
      "TRAINING ENDED\n"
     ]
    }
   ],
   "source": [
    "# Init Q table (Value function)\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Now train the env by processing rewards and learn the Q function\n",
    "episodes = 2000\n",
    "learning_rate = 0.80\n",
    "discount_fact = 0.95\n",
    "rewards_over_episodes = []\n",
    "\n",
    "# Play #episode games\n",
    "for i in range(1, episodes):\n",
    "    accumulated_reward_for_episode = 0\n",
    "    done = False\n",
    "    s = env.reset()  # Reset and get default/ init state.\n",
    "    \n",
    "    for _ in range(99):\n",
    "        \n",
    "        # We are learning now, greedily pick an action while being in state `s`\n",
    "        action = np.argmax(Q[s, :] + (np.random.randn(1, env.action_space.n)*(1./(1 + i))))\n",
    "\n",
    "        # See the consequence of this action\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Alright, we have received reward, let's remember what earned us that amount\n",
    "        # Bellman equation & the fact that we want the updation to be steady and small\n",
    "        Q[s, action] =   Q[s, action] + learning_rate * (reward + (discount_fact * np.max(Q[new_state, :])))\n",
    "                       - learning_rate * Q[s, action]\n",
    "        \n",
    "        # Updating iteration vals\n",
    "        accumulated_reward_for_episode += reward\n",
    "        s = new_state\n",
    "        \n",
    "        # Episode ended\n",
    "        if done is True:\n",
    "            break\n",
    "            \n",
    "    rewards_over_episodes.append(accumulated_reward_for_episode)\n",
    "    if i % 100 == 0:\n",
    "        print \"Reward @ episode %d = %.2f\" % (i, sum(rewards_over_episodes)/episodes)\n",
    "    \n",
    "print \"TRAINING ENDED\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.43567931e-01   1.29953830e-02   1.37789988e-02   1.32270488e-02]\n",
      " [  7.30154553e-04   8.34075080e-04   3.19486029e-04   2.26955196e-01]\n",
      " [  2.92785670e-03   9.91106073e-02   4.55518449e-03   4.24503566e-03]\n",
      " [  6.06637322e-04   5.89402814e-04   0.00000000e+00   4.19002118e-02]\n",
      " [  3.60335085e-01   3.60273873e-03   8.06561456e-03   3.68616199e-03]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  4.28962214e-04   1.14934259e-04   5.71909935e-02   4.39082815e-06]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   5.65256390e-03   4.06607033e-01]\n",
      " [  3.66698844e-03   2.41311287e-01   0.00000000e+00   0.00000000e+00]\n",
      " [  7.74143483e-01   2.15206745e-04   0.00000000e+00   1.26652371e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   7.30694621e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   1.70226393e-02   9.92924781e-01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All good, but what if the env is complex?\n",
    "In a way, that it has waay too many states and waaay too many possible actions?\n",
    "\n",
    "Nothing, there will be a bigger table.\n",
    "But, we have a better method to represent that kind of a big table \n",
    "\n",
    "We will train a Neuralnet to learn this Value function by feeding reward converted as target Q values.\n",
    "Let's do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should be the NeuralNet(NN) architecture? \n",
    "\n",
    "In the above code, we were trying to retrieve value benifits of all possible actions given a state $(s)$. \n",
    "So, this NN should accept state as input and output values (probabilities) of taking each of the four actions.\n",
    "That means, NN will have an input layer of $(1x16)$ and output layer of $(1x4)$. \n",
    "\n",
    "Let there just be one layer in between, my hardware is too slow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-21 15:52:04,272] Making new env: FrozenLake-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Percentage completion = 17.80\n",
      "TRAINING ENDED\n"
     ]
    }
   ],
   "source": [
    "# Let's first define NN\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "# Input layer\n",
    "inputs = tf.placeholder(shape=(1, 16), dtype=tf.float32)\n",
    "\n",
    "# Sandwich layer (1, 16)\n",
    "layer_W = tf.Variable(tf.random_uniform((16, 32), 0, 0.1, dtype=tf.float32))\n",
    "layer_b = tf.Variable(tf.zeros((1, 32), dtype=tf.float32))\n",
    "layer = tf.matmul(inputs, layer_W)\n",
    "\n",
    "# Output layer\n",
    "output_W = tf.Variable(tf.random_uniform((16,4), 0, 0.1), dtype=tf.float32)\n",
    "output_b = tf.Variable(tf.zeros((1, 4), dtype=tf.float32))\n",
    "output = tf.matmul(inputs, output_W)\n",
    "Q = output\n",
    "get_action = tf.argmax(Q, 1)\n",
    "\n",
    "# Error function\n",
    "Q_target = tf.placeholder(shape=(1, 4), dtype=tf.float32)\n",
    "error = Q_target - Q\n",
    "reduced_mean_square_error = tf.reduce_sum(tf.square(error))  # loss function\n",
    "\n",
    "# Minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "back_propagate = optimizer.minimize(reduced_mean_square_error)\n",
    "\n",
    "\n",
    "###################################################################\n",
    "\n",
    "sess = tf.Session()\n",
    "init_global_vars = tf.global_variables_initializer()\n",
    "\n",
    "episodes = 2000\n",
    "discount_fact = 0.95\n",
    "rewards_over_episodes = []\n",
    "sess.run(init_global_vars)\n",
    "\n",
    "# Play #episode games\n",
    "for i in range(1, episodes):\n",
    "    accumulated_reward_for_episode = 0\n",
    "    done = False\n",
    "    s = env.reset()  # Reset and get default/ init state.\n",
    "    \n",
    "    for _ in range(99):\n",
    "        \n",
    "        # We are learning now, greedily pick an action while being in state `s`\n",
    "        # The following lines are that stupidity that kills your PC\n",
    "#        s_one_hot = tf.one_hot(indices=[s], depth=16)\n",
    "#        s_one_hot = sess.run(s_one_hot)\n",
    "        s_one_hot = np.zeros((1, 16))\n",
    "        s_one_hot[0, s] = 1\n",
    "        action, Q_s = sess.run([get_action, Q], {inputs: s_one_hot})\n",
    "        if np.random.rand(1) < 0.15:\n",
    "                action[0] = env.action_space.sample()\n",
    "\n",
    "        # See the consequence of this action\n",
    "        new_state, reward, done, _ = env.step(action[0])\n",
    "        # These too! \n",
    "#        env.render()\n",
    "#        new_state_one_hot = tf.one_hot(indices=[new_state], depth=16)\n",
    "#        new_state_one_hot = sess.run(new_state_one_hot)\n",
    "        new_state_one_hot = np.zeros((1, 16))\n",
    "        new_state_one_hot[0, new_state] = 1\n",
    "        # Alright, we have received reward, let's remember what earned us that amount\n",
    "        # Bellman equation & the fact that we want the updation to be steady and small\n",
    "        Q_new = sess.run(Q, {inputs: new_state_one_hot})\n",
    "        Q_targeted = Q_s\n",
    "        Q_targeted[0, action[0]] = reward + (discount_fact * np.max(Q_new))\n",
    "        sess.run(back_propagate, {Q_target: Q_targeted, inputs: s_one_hot })\n",
    "#        print sess.run(error, {Q_target: Q_targeted, inputs: s_one_hot })\n",
    "        \n",
    "        # Updating iteration vals\n",
    "        accumulated_reward_for_episode += reward\n",
    "        s = new_state\n",
    "        \n",
    "        # Episode ended\n",
    "        if done is True:\n",
    "            break\n",
    "            \n",
    "    rewards_over_episodes.append(accumulated_reward_for_episode)\n",
    "    if i % 1 == 0 and False:\n",
    "        print \"Reward @ episode %d = %.2f\" % (i, sum(rewards_over_episodes)/episodes)\n",
    "print \"Percentage completion = %.2f\" % ((sum(rewards_over_episodes) * 100.0)/ episodes)\n",
    "print \"TRAINING ENDED\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.09951197,  0.12108366,  0.10376973,  0.15691705]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Analyse what this Agent has learned\n",
    "env.render()\n",
    "sess.run(Q, {inputs: [[0, 1, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0]]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f988e8db210>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9894a80390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(rewards_over_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "1. Try other architectures and see how runs-to-learn is changing \n",
    "2. Reduce exploration as we learn better \n",
    "3. Modify env.render() to print inline (Easy to visualize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
