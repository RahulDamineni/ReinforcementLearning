{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Fundamentals of Q learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we start, let's learn about gym\n",
    "\n",
    "    * Gym is an OpenAI platform which provides easy access to various environments. \n",
    "    * Environment (of a game) will have attributes such as `action_space`, `observation_space` & `reward_range`\n",
    "    * We use `step()` method to advance environment when a particular action is chosen\n",
    "    * `step()` returns four outputs; observation, reward, done, info\n",
    "        > Observation is `Discrete`/ `Box` object which gives info about what state env is in currently \n",
    "        > Reward is what we achieved after action\n",
    "        > Done will be true if the env reaches its end state\n",
    "        > Info contains metadata\n",
    "        \n",
    "Read more here: https://gym.openai.com/docs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now a brief about RL \n",
    "\n",
    "Reinforced Learning is all about learning what chain of actions to follow to go through a certain chain of states in order to reach a final state in a way that maximizes our **total** reward\n",
    "\n",
    "### Terminology\n",
    "1. Environment\n",
    "2. Agent\n",
    "3. State\n",
    "4. Observation\n",
    "5. Action\n",
    "6. Reward\n",
    "7. Environment modelling by an Agent\n",
    "8. Policy of an Agent\n",
    "9. Value function of an Agent\n",
    "    * Value function for a fixed action ($\\pi$) for each state $(S)$ : $Q_{\\pi}(S_i)$\n",
    "    * Value function for variable actions for each state $(S)$ : $Q(a_i, S_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The whole drama is for learning $Q(a_i, S_i)$\n",
    "\n",
    "The simplest form of representing this function is using a table. \n",
    "A table of dimension $(#states, #actions)$. Each cell in the table denotes the value of being in a state $s$ and taking an action $a$ in $s$\n",
    "\n",
    "## Frozen lake env in Gym\n",
    "To establish and learn that kind of Q-table, Frozen lake env in Gym would be best fit.\n",
    "This env is 4x4 grid, each cell can be a stable block, a hole or a goal position. \n",
    "Reward to be in any cell except goal cell is zero. Goal cell carries a reward of one.\n",
    "\n",
    "Now possible states in this env is one of the cells, so, 16.\n",
    "Similarly possible actions (left, right, top, bottom) are 4.\n",
    "Let's dive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-21 01:58:51,712] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#states = 16\n",
      "#actions = 4\n"
     ]
    }
   ],
   "source": [
    "print \"#states =\", env.observation_space.n\n",
    "print \"#actions =\", env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward @ episode 100 = 0.00\n",
      "Reward @ episode 200 = 0.02\n",
      "Reward @ episode 300 = 0.04\n",
      "Reward @ episode 400 = 0.06\n",
      "Reward @ episode 500 = 0.09\n",
      "Reward @ episode 600 = 0.11\n",
      "Reward @ episode 700 = 0.14\n",
      "Reward @ episode 800 = 0.17\n",
      "Reward @ episode 900 = 0.21\n",
      "Reward @ episode 1000 = 0.24\n",
      "Reward @ episode 1100 = 0.28\n",
      "Reward @ episode 1200 = 0.31\n",
      "Reward @ episode 1300 = 0.34\n",
      "Reward @ episode 1400 = 0.37\n",
      "Reward @ episode 1500 = 0.41\n",
      "Reward @ episode 1600 = 0.45\n",
      "Reward @ episode 1700 = 0.49\n",
      "Reward @ episode 1800 = 0.52\n",
      "Reward @ episode 1900 = 0.56\n",
      "TRAINING ENDED\n"
     ]
    }
   ],
   "source": [
    "# Init Q table (Value function)\n",
    "\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Now train the env by processing rewards and learn the Q function\n",
    "episodes = 2000\n",
    "learning_rate = 0.80\n",
    "discount_fact = 0.95\n",
    "rewards_over_episodes = []\n",
    "\n",
    "# Play #episode games\n",
    "for i in range(1, episodes):\n",
    "    accumulated_reward_for_episode = 0\n",
    "    done = False\n",
    "    s = env.reset()  # Reset and get default/ init state.\n",
    "    \n",
    "    for _ in range(99):\n",
    "        \n",
    "        # We are learning now, greedily pick an action while being in state `s`\n",
    "        action = np.argmax(Q[s, :] + (np.random.randn(1, env.action_space.n)*(1./(1 + i))))\n",
    "\n",
    "        # See the consequence of this action\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Alright, we have received reward, let's remember what earned us that amount\n",
    "        # Bellman equation & the fact that we want the updation to be steady and small\n",
    "        Q[s, action] =   Q[s, action] + learning_rate * (reward + (discount_fact * np.max(Q[new_state, :])))\n",
    "                       - learning_rate * Q[s, action]\n",
    "        \n",
    "        # Updating iteration vals\n",
    "        accumulated_reward_for_episode += reward\n",
    "        s = new_state\n",
    "        \n",
    "        # Episode ended\n",
    "        if done is True:\n",
    "            break\n",
    "            \n",
    "    rewards_over_episodes.append(accumulated_reward_for_episode)\n",
    "    if i % 100 == 0:\n",
    "        print \"Reward @ episode %d = %.2f\" % (i, sum(rewards_over_episodes)/episodes)\n",
    "    \n",
    "print \"TRAINING ENDED\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.43567931e-01   1.29953830e-02   1.37789988e-02   1.32270488e-02]\n",
      " [  7.30154553e-04   8.34075080e-04   3.19486029e-04   2.26955196e-01]\n",
      " [  2.92785670e-03   9.91106073e-02   4.55518449e-03   4.24503566e-03]\n",
      " [  6.06637322e-04   5.89402814e-04   0.00000000e+00   4.19002118e-02]\n",
      " [  3.60335085e-01   3.60273873e-03   8.06561456e-03   3.68616199e-03]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  4.28962214e-04   1.14934259e-04   5.71909935e-02   4.39082815e-06]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   5.65256390e-03   4.06607033e-01]\n",
      " [  3.66698844e-03   2.41311287e-01   0.00000000e+00   0.00000000e+00]\n",
      " [  7.74143483e-01   2.15206745e-04   0.00000000e+00   1.26652371e-04]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   7.30694621e-01   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   1.70226393e-02   9.92924781e-01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All good, but what if the env is complex?\n",
    "In a way, that it has waay too many states and waaay too many possible actions?\n",
    "\n",
    "Nothing, there will be a bigger table.\n",
    "But, we have a better method to represent that kind of a big table \n",
    "\n",
    "We will train a Neuralnet to learn this Value function by feeding reward converted as target Q values.\n",
    "Let's do that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
